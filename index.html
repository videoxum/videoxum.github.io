<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="Video Summarization, Video Captioning, Cross-modality">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VideoXum: Cross-modal Visual and Textural Summarization of Videos</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">VideoXum: Cross-modal Visual and Textural Summarization of Videos</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://jylin.me">Jingyang Lin</a><sup>1*</sup>,</span>
            <span class="author-block"><a href="https://hanghuacs.owlstown.net/">Hang Hua</a><sup>1*</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=au-RcLAAAAAJ">Ming Chen</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=9UWlxzsAAAAJ">Yikang Li</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=yN-_kP8AAAAJ">Jenhao Hsiao</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=EMms5k8AAAAJ">Chiuman Ho</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><b style="color:#013B68; font-weight:normal">▶ </b><sup>1</sup>University of Rochester</span>
            <span class="author-block"><b style="color:#2D683C; font-weight:normal">▶ </b><sup>2</sup>OPPO US Research Center</span>
          </div>

          <div class="is-size-6">
            <span><i><sup>*</sup> indicates equal contributions</i></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2303.12060"
                   class="external-link button is-normal is-rounded is-danger is-light">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span style="font-weight: bolder;">Paper (arXiv)</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Video summarization aims to distill the most important information from a source video into either an abridged video clip
            or a textual narrative.
            Existing methods often treat the generation of video and text summaries as independent tasks, thus neglecting the semantic
            correlation between visual and textual summarization.
            In other words, these methods only study a single modality as output without considering coherent video and text as outputs.
            In this work, we first introduce a novel task: cross-modal video summarization. This task seeks to transfer a long video into
            a condensed video clip and a semantically aligned textual summary, collectively referred to as a cross-modal summary. 
            We then establish VideoXum (X refers to different modalities), a new large-scale human-annotated video benchmark for cross-modal
            video summarization. VideoXum is reannotated based on ActivityNet Captions with diverse open-domain videos.
            In the current version, VideoXum provides 14K long videos, with a total of 140K pairs of aligned video and text summaries.
            Compared to existing datasets, VideoXum offers superior scalability while preserving a comparable level of annotation quality.
            To validate the dataset's quality, we provide a comprehensive analysis of VideoXum, comparing it with existing datasets.
            Further, we perform an extensive empirical evaluation of several state-of-the-art methods on this dataset. Our findings
            highlight the impressive generalization capability of the vision-language encoder-decoder framework yields on VideoXum.
            Particularly, we propose VTSUM-BLIP, an end-to-end framework, serving as a strong baseline for this novel benchmark.
            Moreover, we adapt CLIPScore for VideoXum to measure the semantic consistency of cross-modal summaries effectively.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Dataset. -->
    <div class="columns is-centered">
      <div class="column is-six-sevenths">
        <h2 class="title is-3">Dataset</h2>
        <div class="content has-text-justified">
          <p>
            In this study, we first propose <span style="font-weight: bolder;">VideoXum</span>, an enriched large-scale
            dataset for cross-modal video summarization. The dataset is built on <a href="https://arxiv.org/abs/1705.00754">ActivityNet Captions</a>,
            a large-scale public video captioning benchmark. We hire workers to annotate ten shortened video summaries
            for each long source video according to the corresponding captions. VideoXum contains 14K long videos with 140K
            pairs of aligned video and text summaries.
          </p>
          <img src="./static/images/v2x-sum.png"
               class="interpolation-image"
               alt="Interpolate start reference image."/>
          <p class="column has-text-centered">
            Illustration of our V2X-SUM task. A long source video (<i>bottom</i>) can be summarized into a shortened videoand
            a text narrative (<i>top</i>). The video and text summaries should be semantically aligned.</p>
          <p>
            Our goal is to extend the traditional single-modal video summarization task to a cross-modal video summarization
            task, referred to as V2X-SUM to meet the demands of broader application scenarios (e.g., movie trailer generation and
            narrative generation). According to the target modality of the generated summaries, we categorize our proposed V2X-SUM
            task into three subtasks:
          </p>
          <ul>
            <li>
              <span style="font-weight: bolder;">Video-to-Video Summarization (V2V-SUM)</span>. This task requires models to identify the most
              important segments from the source video and generate an abridged version of the source video.
            </li>
            <li>
              <span style="font-weight: bolder;">Video-to-Text Summarization (V2T-SUM)</span>. In this task, models need to summarize the main
              content of the source video and generate a short text description.
            </li>
            <li>
              <span style="font-weight: bolder;">Video-to-Video&Text Summarization (V2VT-SUM)</span>. This task requires models to summarize
              a short video and the corresponding narrative from a source video simultaneously. Moreover, the semantics of these two modalities of
              summaries should be well aligned.
            </li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Dataset. -->
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Methodology. -->
    <div class="columns is-centered">
      <div class="column is-six-sevenths">
        <h2 class="title is-3">Methodology</h2>
        <div class="content has-text-justified">
          <p>
            We propose VTSUM-BLIP, a novel end-to-end cross-modal video summarization model.
            To leverage the strong capability of vision understanding and language modeling of pretrained language models,
            we employ <a href="https://arxiv.org/abs/2201.12086">BLIP</a> as our backbone. Then, we design an efficient hierarchical
            video encoding strategy with a frozen encoder and a temporal modeling module to encode long videos. Furthermore,
            we design different task-specific decoders for video and text summarization. The modularized design enables us to
            perform more complex downstream tasks without changing the structure of the pretrained model.
          </p>
        </div>
        <div class="column has-text-centered">
          <img src="./static/images/framework.png"
               class="interpolation-image"
               alt="Interpolate start reference image."/>
          <p class="column has-text-centered">
            An overview of our VTSUM-BLIP model (<i>left</i>). The model consists of a hierarchical video encoder (<i>middle</i>), video-sum decoder,
            and text-sum decoder (<i>right</i>). For V2V-SUM, the video-sum decoder employs a temporal Transformer and local
            self-attention module to aggregate local context. For V2T-SUM, the text-sum decoder is a pretrained BLIP text decoder.
          </p>
        </div>
      </div>
    </div>
    <!--/ Methodology. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Visulization. -->
    <div class="columns is-centered">
      <div class="column is-six-sevenths">
        <h2 class="title is-3">Visulization</h2>
        <div class="content has-text-justified">
          <p>
            We evaluate our model by jointly generating visual and textual summaries for a long video.
            We refer to this task as cross-modal visual and textual summarization of videos (i.e., V2VT-SUM).
            We evaluate our model on VideoXum dataset.
            We show examples qualitative results from the variations of our proposed model (VTSUM-BLIP) in
            the following figure. 
          </p>
        </div>
        <div class="column has-text-centered">
          <img src="./static/images/visualize.png"
               class="interpolation-image"
               alt="Interpolate start reference image."/>
          <p class="column has-text-centered">
            Example results of the generated video and text summaries across different baseline models. Red (both line and box)
            indicates the results of the ground truth. Green indicates the results of the VTSUM-BLIP (Base). Blue indicates the
            results of VTSUM-BLIP (+TT+CA).
          </p>
        </div>
      </div>
    </div>
    <!--/ Visulization. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">Citation</h2>
    <pre><code>@article{lin2023videoxum,
  author    = {Lin, Jingyang and Hua, Hang and Chen, Ming and Li, Yikang and Hsiao, Jenhao and Ho, Chiuman and Luo, Jiebo},
  title     = {VideoXum: Cross-modal Visual and Textural Summarization of Videos},
  journal   = {IEEE Transactions on Multimedia},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <!-- <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p> -->
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
