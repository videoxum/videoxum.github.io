<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="Video Summarization, Video Captioning, Cross-modality">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VideoXum: Cross-modal Visual and Textural Summarization of Videos</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">VideoXum: Cross-modal Visual and Textural Summarization of Videos</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://jylin.me">Jingyang Lin</a><sup>1*</sup>,</span>
            <span class="author-block"><a href="https://hanghuacs.owlstown.net/">Hang Hua</a><sup>1*</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=au-RcLAAAAAJ">Ming Chen</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=9UWlxzsAAAAJ">Yikang Li</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=yN-_kP8AAAAJ">Jenhao Hsiao</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://scholar.google.com/citations?user=EMms5k8AAAAJ">Chiuman Ho</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><b style="color:#013B68; font-weight:normal">▶ </b><sup>1</sup>University of Rochester</span>
            <span class="author-block"><b style="color:#2D683C; font-weight:normal">▶ </b><sup>2</sup>OPPO US Research Center</span>
          </div>

          <div class="is-size-6">
            <span><i><sup>*</sup> indicates equal contributions</i></span>
          </div>

          <div class="publication-links">
            <span class="link-block">
              <a href="https://arxiv.org/abs/2303.12060" target="_blank" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
              </a>
            </span>
            <span class="link-block">
              <a href="https://github.com/jylins/videoxum" target="_blank" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
                </span>
                <span>Code</span>
              </a>
            </span>
            </span>
            <span class="link-block">
              <a href="https://huggingface.co/datasets/jylins/videoxum" target="_blank" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <svg class="svg-inline--fa fa-database fa-w-14" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="database" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" data-fa-i2svg=""><path fill="currentColor" d="M448 73.143v45.714C448 159.143 347.667 192 224 192S0 159.143 0 118.857V73.143C0 32.857 100.333 0 224 0s224 32.857 224 73.143zM448 176v102.857C448 319.143 347.667 352 224 352S0 319.143 0 278.857V176c48.125 33.143 136.208 48.572 224 48.572S399.874 209.143 448 176zm0 160v102.857C448 479.143 347.667 512 224 512S0 479.143 0 438.857V336c48.125 33.143 136.208 48.572 224 48.572S399.874 369.143 448 336z"></path></svg><!-- <i class="fas fa-database"></i> Font Awesome fontawesome.com -->
                </span>
                <span>Dataset</span>
              </a>
            </span>
            <span class="link-block">
              <a href="https://huggingface.co/jylins/vtsum_blip" target="_blank" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <svg class="svg-inline--fa fa-share-square fa-w-18" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="share-square" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512" data-fa-i2svg=""><path fill="currentColor" d="M568.482 177.448L424.479 313.433C409.3 327.768 384 317.14 384 295.985v-71.963c-144.575.97-205.566 35.113-164.775 171.353 4.483 14.973-12.846 26.567-25.006 17.33C155.252 383.105 120 326.488 120 269.339c0-143.937 117.599-172.5 264-173.312V24.012c0-21.174 25.317-31.768 40.479-17.448l144.003 135.988c10.02 9.463 10.028 25.425 0 34.896zM384 379.128V448H64V128h50.916a11.99 11.99 0 0 0 8.648-3.693c14.953-15.568 32.237-27.89 51.014-37.676C185.708 80.83 181.584 64 169.033 64H48C21.49 64 0 85.49 0 112v352c0 26.51 21.49 48 48 48h352c26.51 0 48-21.49 48-48v-88.806c0-8.288-8.197-14.066-16.011-11.302a71.83 71.83 0 0 1-34.189 3.377c-7.27-1.046-13.8 4.514-13.8 11.859z"></path></svg><!-- <i class="fas fa-share-square"></i> Font Awesome fontawesome.com -->
                </span>
                <span>Model</span>
              </a>
            </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Video summarization aims to distill the most important information from a source video into either an abridged video clip
            or a textual narrative.
            Existing methods often treat the generation of video and text summaries as independent tasks, thus neglecting the semantic
            correlation between visual and textual summarization.
            In other words, these methods only study a single modality as output without considering coherent video and text as outputs.
            In this work, we first introduce a novel task: cross-modal video summarization. This task seeks to transfer a long video into
            a condensed video clip and a semantically aligned textual summary, collectively referred to as a cross-modal summary. 
            We then establish VideoXum (X refers to different modalities), a new large-scale human-annotated video benchmark for cross-modal
            video summarization. VideoXum is reannotated based on ActivityNet Captions with diverse open-domain videos.
            In the current version, VideoXum provides 14K long videos, with a total of 140K pairs of aligned video and text summaries.
            Compared to existing datasets, VideoXum offers superior scalability while preserving a comparable level of annotation quality.
            To validate the dataset's quality, we provide a comprehensive analysis of VideoXum, comparing it with existing datasets.
            Further, we perform an extensive empirical evaluation of several state-of-the-art methods on this dataset. Our findings
            highlight the impressive generalization capability of the vision-language encoder-decoder framework yields on VideoXum.
            Particularly, we propose VTSUM-BLIP, an end-to-end framework, serving as a strong baseline for this novel benchmark.
            Moreover, we adapt CLIPScore for VideoXum to measure the semantic consistency of cross-modal summaries effectively.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Dataset. -->
    <div class="columns is-centered">
      <div class="column is-six-sevenths">
        <h2 class="title is-3">Dataset</h2>
        <div class="content has-text-justified">
          <p>
            In this study, we first propose <span style="font-weight: bolder;">VideoXum</span>, an enriched large-scale
            dataset for cross-modal video summarization. The dataset is built on <a href="https://arxiv.org/abs/1705.00754">ActivityNet Captions</a>,
            a large-scale public video captioning benchmark. We hire workers to annotate ten shortened video summaries
            for each long source video according to the corresponding captions. VideoXum contains 14K long videos with 140K
            pairs of aligned video and text summaries.
          </p>
          <img src="./static/images/v2x-sum.png"
               class="interpolation-image"
               alt="Interpolate start reference image."/>
          <p class="column has-text-centered">
            Illustration of our V2X-SUM task. A long source video (<i>bottom</i>) can be summarized into a shortened videoand
            a text narrative (<i>top</i>). The video and text summaries should be semantically aligned.</p>
          <p>
            Our goal is to extend the traditional single-modal video summarization task to a cross-modal video summarization
            task, referred to as V2X-SUM to meet the demands of broader application scenarios (e.g., movie trailer generation and
            narrative generation). According to the target modality of the generated summaries, we categorize our proposed V2X-SUM
            task into three subtasks:
          </p>
          <ul>
            <li>
              <span style="font-weight: bolder;">Video-to-Video Summarization (V2V-SUM)</span>. This task requires models to identify the most
              important segments from the source video and generate an abridged version of the source video.
            </li>
            <li>
              <span style="font-weight: bolder;">Video-to-Text Summarization (V2T-SUM)</span>. In this task, models need to summarize the main
              content of the source video and generate a short text description.
            </li>
            <li>
              <span style="font-weight: bolder;">Video-to-Video&Text Summarization (V2VT-SUM)</span>. This task requires models to summarize
              a short video and the corresponding narrative from a source video simultaneously. Moreover, the semantics of these two modalities of
              summaries should be well aligned.
            </li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Dataset. -->
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Methodology. -->
    <div class="columns is-centered">
      <div class="column is-six-sevenths">
        <h2 class="title is-3">Methodology</h2>
        <div class="content has-text-justified">
          <p>
            We propose VTSUM-BLIP, a novel end-to-end cross-modal video summarization model.
            To leverage the strong capability of vision understanding and language modeling of pretrained language models,
            we employ <a href="https://arxiv.org/abs/2201.12086">BLIP</a> as our backbone. Then, we design an efficient hierarchical
            video encoding strategy with a frozen encoder and a temporal modeling module to encode long videos. Furthermore,
            we design different task-specific decoders for video and text summarization. The modularized design enables us to
            perform more complex downstream tasks without changing the structure of the pretrained model.
          </p>
        </div>
        <div class="column has-text-centered">
          <img src="./static/images/framework.png"
               class="interpolation-image"
               alt="Interpolate start reference image."/>
          <p class="column has-text-centered">
            An overview of our VTSUM-BLIP model (<i>left</i>). The model consists of a hierarchical video encoder (<i>middle</i>), video-sum decoder,
            and text-sum decoder (<i>right</i>). For V2V-SUM, the video-sum decoder employs a temporal Transformer and local
            self-attention module to aggregate local context. For V2T-SUM, the text-sum decoder is a pretrained BLIP text decoder.
          </p>
        </div>
      </div>
    </div>
    <!--/ Methodology. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Visulization. -->
    <div class="columns is-centered">
      <div class="column is-six-sevenths">
        <h2 class="title is-3">Visulization</h2>
        <div class="content has-text-justified">
          <p>
            We evaluate our model by jointly generating visual and textual summaries for a long video.
            We refer to this task as cross-modal visual and textual summarization of videos (i.e., V2VT-SUM).
            We evaluate our model on VideoXum dataset.
            We show examples qualitative results from the variations of our proposed model (VTSUM-BLIP) in
            the following figure. 
          </p>
        </div>
        <div class="column has-text-centered">
          <img src="./static/images/visualize.png"
               class="interpolation-image"
               alt="Interpolate start reference image."/>
          <p class="column has-text-centered">
            Example results of the generated video and text summaries across different baseline models. Red (both line and box)
            indicates the results of the ground truth. Green indicates the results of the VTSUM-BLIP (Base). Blue indicates the
            results of VTSUM-BLIP (+TT+CA).
          </p>
        </div>
      </div>
    </div>
    <!--/ Visulization. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">Citation</h2>
    <pre><code>@article{lin2023videoxum,
  author    = {Lin, Jingyang and Hua, Hang and Chen, Ming and Li, Yikang and Hsiao, Jenhao and Ho, Chiuman and Luo, Jiebo},
  title     = {VideoXum: Cross-modal Visual and Textural Summarization of Videos},
  journal   = {IEEE Transactions on Multimedia},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <!-- <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p> -->
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
